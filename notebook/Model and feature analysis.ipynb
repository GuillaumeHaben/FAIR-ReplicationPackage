{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_json('dataset.35past.Windows.json')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "# CLEAN DATA\n",
    "\n",
    "# Remove test_fail \n",
    "# data = data[~data[\"testId\"].isin([\"telemetry_unittests_ninja://chrome/test:telemetry_unittests/unit_tests_test.ExampleTests.test_fail\"])]\n",
    "\n",
    "# Keep flaky failures and legit failures only\n",
    "data = data[((data[\"flakeRate\"] == 0) & (data[\"label\"] == 1)) | (data[\"label\"] == 0)]\n",
    "# data = data.drop(data[((data['flakeRate'] == 0) & (data[\"label\"] == 0))].sample(frac=.95).index)\n",
    "\n",
    "# Drop test duplicates\n",
    "# data = data.drop_duplicates(subset=[\"testId\"])\n",
    "data = data.replace([np.inf, -np.inf], np.nan).dropna(axis=1)\n",
    "\n",
    "# Specific test suite\n",
    "# Linux. GUI -> GUI \n",
    "# data = data[data[\"testSuite\"].isin([\"blink_web_tests\", \"not_site_per_process_blink_web_tests\", \"vulkan_swiftshader_blink_web_tests\", \"non_skia_renderer_swiftshader_blink_web_tests\", \"interactive_ui_tests\"])]\n",
    "# Linux. Integration -> Integration \n",
    "# data = data[data[\"testSuite\"].isin([\"browser_tests\",\"sync_integration_tests\",\"content_browsertests\",\"weblayer_browsertests\",\"extensions_browsertests\",\"headless_browsertests\",\"components_browsertests\"])]\n",
    "# Linux. Unit -> Unit \n",
    "data = data[~data[\"testSuite\"].isin([\"browser_tests\",\"sync_integration_tests\",\"content_browsertests\",\"weblayer_browsertests\",\"extensions_browsertests\",\"headless_browsertests\",\"components_browsertests\", \"blink_web_tests\", \"not_site_per_process_blink_web_tests\", \"vulkan_swiftshader_blink_web_tests\", \"non_skia_renderer_swiftshader_blink_web_tests\", \"interactive_ui_tests\"])]\n",
    "\n",
    "# Windows. GUI -> GUI \n",
    "# data = data[data[\"testSuite\"].isin([\"blink_web_tests\", \"pixel_browser_tests\", \"interactive_ui_tests\", \"non_skia_renderer_content_browsertests\"])]\n",
    "# Windows. Integration -> Integration \n",
    "# data = data[data[\"testSuite\"].isin([\"browser_tests\",\"content_browsertests\",\"sbox_integration_tests\",\"extensions_browsertests\",\"cronet_tests\",\"sync_integration_tests\",\"headless_browsertests\"])]\n",
    "# Windows. Unit -> Unit \n",
    "# data = data[~data[\"testSuite\"].isin([\"browser_tests\",\"content_browsertests\",\"sbox_integration_tests\",\"extensions_browsertests\",\"cronet_tests\",\"sync_integration_tests\",\"headless_browsertests\", \"blink_web_tests\", \"pixel_browser_tests\", \"interactive_ui_tests\", \"non_skia_renderer_content_browsertests\"])]\n",
    "\n",
    "# Window-based\n",
    "# Linux first 500\n",
    "# data = data[(data[\"buildId\"] >= 98019) & (data[\"buildId\"] <= 98419)]\n",
    "# Linux last 500\n",
    "# data = data[(data[\"buildId\"] > 98419)]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(Counter(data[\"label\"]))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer, Binarizer\n",
    "\n",
    "# Columns definition\n",
    "testSource_col = \"testSource\"\n",
    "stackTrace_col = \"stackTrace\"\n",
    "command_col = \"command\"\n",
    "stderr_col = \"stderr\"\n",
    "crashlog_col = \"crashlog\"\n",
    "cat_cols = [\"runStatus\", \"runTagStatus\", \"testSuite\"]\n",
    "std_cols = [\"flakeRate\"]\n",
    "bin_cols = ['stackTraceLength', 'commandLength', 'stderrLength', 'crashlogLength', 'testSourceLength']\n",
    "other_cols = [\"runDuration\"]\n",
    "\n",
    "# Fair features\n",
    "X = data[cat_cols + other_cols + std_cols + [testSource_col, stackTrace_col, command_col, stderr_col, crashlog_col]]\n",
    "y = data['label']\n",
    "\n",
    "# Columns transformers\n",
    "cols_trans = ColumnTransformer([\n",
    "    ('categories', OneHotEncoder(handle_unknown = \"ignore\"), cat_cols),\n",
    "    ('stackTrace', TfidfVectorizer(max_features=100), stackTrace_col),\n",
    "    ('command', TfidfVectorizer(max_features=100), command_col),\n",
    "    ('stderr', TfidfVectorizer(max_features=100), stderr_col),\n",
    "    ('crashlog', TfidfVectorizer(max_features=100), crashlog_col),\n",
    "    ('testSource', TfidfVectorizer(max_features=100), testSource_col),\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "# Voc features\n",
    "# X = data[[testSource_col]]\n",
    "# y = data['label']\n",
    "\n",
    "# # # Columns transformers\n",
    "# cols_trans = ColumnTransformer([\n",
    "#     ('testSource', CountVectorizer(), testSource_col),\n",
    "#     ], remainder='drop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split Train and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "timeSensitive = False\n",
    "\n",
    "# TIME SENSITIVE\n",
    "if timeSensitive:\n",
    "    ts = 0.20\n",
    "    y_flaky = y[y == 0]\n",
    "    y_legit = y[y == 1]\n",
    "    X_legit = X.iloc[y_legit.index, :]\n",
    "    X_flaky = X.iloc[y_flaky.index, :]\n",
    "\n",
    "    X_flaky_train, X_flaky_test, y_flaky_train, y_flaky_test = train_test_split(X_flaky, y_flaky, test_size=ts, shuffle=False)\n",
    "    X_legit_train, X_legit_test, y_legit_train, y_legit_test = train_test_split(X_legit, y_legit, test_size=ts, shuffle=False)\n",
    "\n",
    "    X_train = pd.concat([X_flaky_train, X_legit_train])\n",
    "    y_train = pd.concat([y_flaky_train, y_legit_train])\n",
    "    X_test = pd.concat([X_flaky_test, X_legit_test])\n",
    "    y_test = pd.concat([y_flaky_test, y_legit_test])\n",
    "\n",
    "# NORMAL SPLIT\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y)\n",
    "    \n",
    "# GENERAL INFO\n",
    "print(\"Train set:\")\n",
    "print(Counter(y_train))\n",
    "print(\"Test set:\")\n",
    "print(Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID SEARCH\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Scoring functions\n",
    "# def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "# def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "# def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "# def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n",
    "# def recall(y_true, y_pred): return tp(y_true, y_pred) / (tp(y_true, y_pred) + fn(y_true, y_pred))\n",
    "# def precision(y_true, y_pred): return tp(y_true, y_pred) / (tp(y_true, y_pred) + fp(y_true, y_pred))\n",
    "# def f1(y_true, y_pred): return 2*tp(y_true, y_pred) / (2*tp(y_true, y_pred) + fp(y_true, y_pred) + fn(y_true, y_pred))\n",
    "# def mcc(y_true, y_pred): return (tp(y_true, y_pred) * tn(y_true, y_pred) - fp(y_true, y_pred) * fn(y_true, y_pred)) / np.sqrt((tp(y_true, y_pred) + fp(y_true, y_pred)) * (tp(y_true, y_pred) + fn(y_true, y_pred)) * (tn(y_true, y_pred) + fp(y_true, y_pred)) * (tn(y_true, y_pred) + fn(y_true, y_pred)))\n",
    "# def displayScores(scores, title):\n",
    "#     print(\"\\nMetric: \", title)\n",
    "#     print(\"Scores: \", scores)\n",
    "#     print(\"Accuracy: %0.2f (+/- %0.2f)\" % (np.nanmean(scores), np.nanstd(scores) * 2))\n",
    "\n",
    "# # Model with cross validation\n",
    "# scoring = {\n",
    "#     'precision': make_scorer(precision), \n",
    "#     'recall': make_scorer(recall), \n",
    "#     'f1': make_scorer(f1), \n",
    "#     'mcc': make_scorer(mcc)\n",
    "# }\n",
    "\n",
    "# # scores = cross_validate(pipe, X, y, cv=5, scoring=scoring, n_jobs=14)\n",
    "# # displayScores(scores['test_precision'], \"Precision\")\n",
    "# # displayScores(scores['test_recall'], \"Recall\")\n",
    "# # displayScores(scores['test_f1'], \"F1\")\n",
    "# # displayScores(scores['test_mcc'], \"MCC\")\n",
    "\n",
    "# # Grid search\n",
    "# param_grid = {\n",
    "#     \"fs__k\": [50, 100, 200]\n",
    "# }\n",
    "# search = GridSearchCV(pipe, param_grid, cv=2, verbose=1, n_jobs=14)\n",
    "# search.fit(X_train, y_train)\n",
    "# print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "# print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMAL PIPELINE\n",
    "from numpy import mean\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, r2_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier \n",
    "\n",
    "# Pipeline\n",
    "smote = SMOTE(sampling_strategy=1)\n",
    "featureSelection = SelectKBest(chi2, k=200)\n",
    "rfc = BalancedRandomForestClassifier(n_estimators=100)\n",
    "\n",
    "steps = [\n",
    "    ('trans', cols_trans),\n",
    "    ('fs', featureSelection),\n",
    "    ('s', smote), \n",
    "    ('m', rfc)\n",
    "]\n",
    "pipe = Pipeline(steps=steps)\n",
    "display(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected features\n",
    "mask = pipe.named_steps[\"fs\"].get_support() #list of booleans\n",
    "new_features = [] # The list of your K best features\n",
    "feature_names = pipe.named_steps[\"trans\"].get_feature_names()\n",
    "print(len(feature_names))\n",
    "for bool, feature in zip(mask, feature_names):\n",
    "    if bool:\n",
    "        new_features.append(feature)\n",
    "pprint(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scores\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nPrecision\", precision)\n",
    "print(\"Recall\", recall)\n",
    "print(\"MCC\", mcc)\n",
    "print(\"F1\", f1)\n",
    "print(\"R2\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Features importance\n",
    "print(\"Number of features before feature selection:\", len(pipe.named_steps[\"trans\"].get_feature_names()))\n",
    "# In case of no feature selection\n",
    "# zipped = zip(pipe.named_steps[\"trans\"].get_feature_names(), pipe.named_steps[\"m\"].feature_importances_)\n",
    "\n",
    "# In case of feature selection\n",
    "zipped = zip(new_features, pipe.named_steps[\"m\"].feature_importances_)\n",
    "\n",
    "zipped = sorted(zipped, key=lambda x: x[1], reverse=True)\n",
    "for feature, importance in zipped:\n",
    "    print('{}: {}'.format(feature, importance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
